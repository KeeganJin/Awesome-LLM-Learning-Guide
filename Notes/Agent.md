
## 1. Agent 智能体经典范式
### 1.1. ReAct
1）ReAct 的主要特点

高可解释性：ReAct 最大的优点之一就是透明。通过 Thought 链，我们可以清晰地看到智能体每一步的“心路历程”——它为什么会选择这个工具，下一步又打算做什么。这对于理解、信任和调试智能体的行为至关重要。
动态规划与纠错能力：与一次性生成完整计划的范式不同，ReAct 是“走一步，看一步”。它根据每一步从外部世界获得的 Observation 来动态调整后续的 Thought 和 Action。如果上一步的搜索结果不理想，它可以在下一步中修正搜索词，重新尝试。
工具协同能力：ReAct 范式天然地将大语言模型的推理能力与外部工具的执行能力结合起来。LLM 负责运筹帷幄（规划和推理），工具负责解决具体问题（搜索、计算），二者协同工作，突破了单一 LLM 在知识时效性、计算准确性等方面的固有局限。
（2）ReAct 的固有局限性

对LLM自身能力的强依赖：ReAct 流程的成功与否，高度依赖于底层 LLM 的综合能力。如果 LLM 的逻辑推理能力、指令遵循能力或格式化输出能力不足，就很容易在 Thought 环节产生错误的规划，或者在 Action 环节生成不符合格式的指令，导致整个流程中断。
执行效率问题：由于其循序渐进的特性，完成一个任务通常需要多次调用 LLM。每一次调用都伴随着网络延迟和计算成本。对于需要很多步骤的复杂任务，这种串行的“思考-行动”循环可能会导致较高的总耗时和费用。
提示词的脆弱性：整个机制的稳定运行建立在一个精心设计的提示词模板之上。模板中的任何微小变动，甚至是用词的差异，都可能影响 LLM 的行为。此外，并非所有模型都能持续稳定地遵循预设的格式，这增加了在实际应用中的不确定性。
可能陷入局部最优：步进式的决策模式意味着智能体缺乏一个全局的、长远的规划。它可能会因为眼前的 Observation 而选择一个看似正确但长远来看并非最优的路径，甚至在某些情况下陷入“原地打转”的循环中。
（3）调试技巧

当你构建的 ReAct 智能体行为不符合预期时，可以从以下几个方面入手进行调试：

检查完整的提示词：在每次调用 LLM 之前，将最终格式化好的、包含所有历史记录的完整提示词打印出来。这是追溯 LLM 决策源头的最直接方式。
分析原始输出：当输出解析失败时（例如，正则表达式没有匹配到 Action），务必将 LLM 返回的原始、未经处理的文本打印出来。这能帮助你判断是 LLM 没有遵循格式，还是你的解析逻辑有误。
验证工具的输入与输出：检查智能体生成的 tool_input 是否是工具函数所期望的格式，同时也要确保工具返回的 observation 格式是智能体可以理解和处理的。
调整提示词中的示例 (Few-shot Prompting)：如果模型频繁出错，可以在提示词中加入一两个完整的“Thought-Action-Observation”成功案例，通过示例来引导模型更好地遵循你的指令。
尝试不同的模型或参数：更换一个能力更强的模型，或者调整 temperature 参数（通常设为0以保证输出的确定性），有时能直接解决问题


### 1.2. Plan-and-Solve



### 1.3. Reflection
* Post-hoc 事后的自我校正循环
* 执行，反思，优化

1. 执行 (Execution)：首先，智能体使用我们熟悉的方法（如 ReAct 或 Plan-and-Solve）尝试完成任务，生成一个初步的解决方案或行动轨迹。这可以看作是“初稿”。
2. 反思 (Reflection)：接着，智能体进入反思阶段。它会调用一个独立的、或者带有特殊提示词的大语言模型实例，来扮演一个“评审员”的角色。这个“评审员”会审视第一步生成的“初稿”，并从多个维度进行评估，例如：
   * 事实性错误：是否存在与常识或已知事实相悖的内容？
   * 逻辑漏洞：推理过程是否存在不连贯或矛盾之处？
   * 效率问题：是否有更直接、更简洁的路径来完成任务？
   * 遗漏信息：是否忽略了问题的某些关键约束或方面？ 根据评估，它会生成一段结构化的反馈 (Feedback)，指出具体的问题所在和改进建议。
3. Refinement


## 2. Frameworks

### 2.1. LangGraph
LangGraph 将智能体的执行流程建模为一种**状态机**（State Machine），将其表示为有向图（Directed Graph）。在这种范式中，图的节点（Nodes）代表一个具体的计算步骤（如调用 LLM、执行工具），而边（Edges）则定义了从一个节点到另一个节点的跳转逻辑。

要理解 LangGraph，我们需要先掌握它的三个基本构成要素。
* 全局变量（State）
  * 整个图的执行过程都围绕一个共享的状态对象进行。这个状态通常被定义为一个 Python 的 TypedDict，它可以包含任何你需要追踪的信息，如对话历史、中间结果、迭代次数等。所有的节点都能读取和更新这个中心状态。
* 节点（Nodes）
  * 每个节点都是一个接收当前状态作为输入、并返回一个更新后的状态作为输出的 Python 函数。节点是执行具体工作的单元。
* 边（Edge）
  * 边负责连接节点，定义工作流的方向。最简单的边是常规边，它指定了一个节点的输出总是流向另一个固定的节点。而 LangGraph 最强大的功能在于条件边（Conditional Edges）。它通过一个函数来判断当前的状态，然后动态地决定下一步应该跳转到哪个节点。这正是实现循环和复杂逻辑分支的关键。

#### 2.1.1. 优势


#### 2.1.2. 局限性


## 3. 8 Memory and Retrieval

### 3.1. 8.1 Overview
### 3.2. Why Agent needs Memory and RAG
* 无状态导致的状态遗忘
* 模型内置知识的局限性

### 3.3. Memory and RAG System Architecture Design

### 3.4. 四种记忆类型
* 工作记忆 Working Memory
  * 工作记忆是记忆系统中最活跃的部分，它负责存储当前对话会话中的临时信息。工作记忆的设计重点在于快速访问和自动清理，这种设计确保了系统的响应速度和资源效率。
  * 工作记忆采用了纯内存存储方案，配合TTL（Time To Live）机制进行自动清理。这种设计的优势在于访问速度极快，但也意味着工作记忆的内容在系统重启后会丢失。这种特性正好符合工作记忆的定位，存储临时的、易变的信息
* 情境记忆 Episodic Memory
  * 情景记忆负责存储具体的事件和经历，它的设计重点在于保持事件的完整性和时间序列关系。情景记忆采用了SQLite+Qdrant的混合存储方案，SQLite负责结构化数据的存储和复杂查询，Qdrant负责高效的向量检索。

### 3.5. 8.2 Memory System


| 记忆类型                         | 存什么？         | 保存多久？  | 加载方式                 | 应用场景      |
| ---------------------------- | ------------ | ------ | -------------------- | --------- |
| **短期记忆（Working Memory）**     | 当前对话、临时变量    | 会话级别   | 上下文直接注入              | 多轮对话理解    |
| **长期记忆（Long-Term Memory）**   | 稳定知识、用户偏好    | 持久存储   | 向量检索/RAG             | 用户画像、累积知识 |
| **语义记忆（Semantic Memory）**    | 客观事实、通用知识    | 持久     | 向量搜索 + RAG           | 信息检索与知识回答 |
| **情景记忆（Episodic Memory）**    | 事件与任务经历      | 长期     | 时间序列索引               | 行动回顾、规避错误 |
| **程序性记忆（Procedural Memory）** | 工具调用经验、策略    | 长期     | Policy 提取、Reflection | 行动规划、自我改进 |
| **意图/目标记忆（Goal Memory）**     | 用户意图、Agent目标 | 会话级/长期 | 状态管理器                | 连续任务执行    |

**一下我们主要记录几个比较常用的memory**
#### 3.5.1. 8.2.5 四种记忆类型
（1）工作记忆（WorkingMemory）

工作记忆是记忆系统中最活跃的部分，它负责存储当前对话会话中的临时信息。工作记忆的设计重点在于快速访问和自动清理，这种设计确保了系统的响应速度和资源效率。

工作记忆采用了纯内存存储方案，配合TTL（Time To Live）机制进行自动清理。这种设计的优势在于访问速度极快，但也意味着工作记忆的内容在系统重启后会丢失。这种特性正好符合工作记忆的定位，存储临时的、易变的信息。

（2）情景记忆（EpisodicMemory）

情景记忆负责存储具体的事件和经历，它的设计重点在于保持事件的完整性和时间序列关系。情景记忆采用了SQLite+Qdrant的混合存储方案，SQLite负责结构化数据的存储和复杂查询，Qdrant负责高效的向量检索。

（3）语义记忆（SemanticMemory）

语义记忆是记忆系统中最复杂的部分，它负责存储抽象的概念、规则和知识。语义记忆的设计重点在于知识的结构化表示和智能推理能力。语义记忆采用了Neo4j图数据库和Qdrant向量数据库的混合架构，这种设计让系统既能进行快速的语义检索，又能利用知识图谱进行复杂的关系推理。

语义记忆的添加过程体现了知识图谱构建的完整流程。系统不仅存储记忆内容，还会自动提取实体和关系，构建结构化的知识表示：

（4）感知记忆（PerceptualMemory）

感知记忆支持文本、图像、音频等多种模态的数据存储和检索。它采用了模态分离的存储策略，为不同模态的数据创建独立的向量集合，这种设计避免了维度不匹配的问题，同时保证了检索的准确性：
感知记忆的检索支持同模态和跨模态两种模式。同模态检索利用专业的编码器进行精确匹配，而跨模态检索则需要更复杂的语义对齐机制


#### 3.5.2. 8.3 RAG系统
（3）发展历程

第一阶段：朴素RAG（Naive RAG, 2020-2021）。这是RAG技术的萌芽阶段，其流程直接而简单，通常被称为“检索-读取”（Retrieve-Read）模式。检索方式：主要依赖传统的关键词匹配算法，如TF-IDF或BM25。这些方法计算词频和文档频率来评估相关性，对字面匹配效果好，但难以理解语义上的相似性。生成模式：将检索到的文档内容不加处理地直接拼接到提示词的上下文中，然后送给生成模型。

第二阶段：高级RAG（Advanced RAG, 2022-2023）。随着向量数据库和文本嵌入技术的成熟，RAG进入了快速发展阶段。研究者和开发者们在“检索”和“生成”的各个环节引入了大量优化技术。检索方式：转向基于稠密嵌入（Dense Embedding）的语义检索。通过将文本转换为高维向量，模型能够理解和匹配语义上的相似性，而不仅仅是关键词。生成模式：引入了很多优化技术，例如查询重写，文档分块，重排序等。

第三阶段：模块化RAG（Modular RAG, 2023-至今）。在高级RAG的基础上，现代RAG系统进一步向着模块化、自动化和智能化的方向发展。系统的各个部分被设计成可插拔、可组合的独立模块，以适应更多样化和复杂的应用场景。检索方式：如混合检索，多查询扩展，假设性文档嵌入等。生成模式：思维链推理，自我反思与修正等。


##### 3.5.2.1. 8.3.4 RAG系统架构设计

##### 3.5.2.2. 8.3.5 高级检索策略

1. 多查询扩展（MQE）
多查询扩展（Multi-Query Expansion）是一种通过生成语义等价的多样化查询来提高检索召回率的技术。这种方法的核心洞察是：同一个问题可以有多种不同的表述方式，而不同的表述可能匹配到不同的相关文档。例如，"如何学习Python"可以扩展为"Python入门教程"、"Python学习方法"、"Python编程指南"等多个查询。通过并行执行这些扩展查询并合并结果，系统能够覆盖更广泛的相关文档，避免因用词差异而遗漏重要信息。
2. 假设文档嵌入（HyDE）
假设文档嵌入（Hypothetical Document Embeddings，HyDE）是一种创新的检索技术，它的核心思想是"用答案找答案"。传统的检索方法是用问题去匹配文档，但问题和答案在语义空间中的分布往往存在差异——问题通常是疑问句，而文档内容是陈述句。HyDE通过让LLM先生成一个假设性的答案段落，然后用这个答案段落去检索真实文档，从而缩小了查询和文档之间的语义鸿沟。


## 4. 9 上下文工程


## 5. 智能体通信协议
* MCP: 智能体与工具的交互
* 
* A2A - Agent to Agent
* 
* ANP Agent Network Protocol


### 5.1. 10.4 ANP 协议

* 服务发现：当新任务到达时，如何快速找到能够处理该任务的智能体？
* 智能路由：如果多个智能体都能处理同一任务，如何选择最合适的一个（如根据负载、成本等）并向其分派任务？
* 动态扩展：如何让新加入网络的智能体被其他成员发现和调用？

### 5.2. 10.5 构建自定义MCP服务器



## 6. 11 Agentic RL


#### 6.0.1. Agent RL 设计

这里我们参考 TRL (Transformer Reinforcement Learning)


### 6.1. 数据集和奖励函数

#### 6.1.1. 11.2.2 奖励系统设置
常用的包含以下， 常常组合使用：e.g., 为了获得步骤奖励，生成可能冗长，结合长度奖励，控制其长度
* 准确率奖励
* 长度惩罚
* 步骤奖励
![alt text](11-table-4.png)


### 6.2. SFT训练

#### 6.2.1. LoRA: 参数高效微调
接微调整个模型需要大量的计算资源和显存。对于 Qwen3-0.6B(0.6B 参数)，全量微调需要约 12GB 显存(FP16)或 24GB 显存(FP32)。对于更大的模型(如 7B、13B)，全量微调几乎不可能在消费级 GPU 上进行。


 LoRA 的优势:显存占用大幅降低、训练速度更快、易于部署、防止过拟合。不过训练的效果通常情况会比全量调参更差一些

 LoRA 的关键超参数包括：

- **秩（rank，r）**：控制 LoRA 矩阵的秩，越大表达能力越强，但参数量也越多，典型值为 4–64，默认 8。
- **Alpha（α）**：LoRA 的缩放因子，实际更新为  
  \[
  \Delta W = \frac{\alpha}{r} BA
  \]
  控制 LoRA 的影响强度，典型值等于 rank。
- **目标模块（target modules）**：指定哪些层应用 LoRA，通常选择注意力层（`q_proj`、`k_proj`、`v_proj`、`o_proj`），也可以包括 MLP 层（`gate_proj`、`up_proj`、`down_proj`）。

##### 6.2.1.1. 训练监控和调试

在训练过程中，我们需要监控三个关键指标。损失(Loss)应该逐渐下降，如果不下降可能是学习率太小或数据有问题，如果下降后又上升则可能是学习率太大或出现过拟合。梯度范数(Gradient Norm)应该在 0.1-10 的合理范围内，过大(>100)说明出现梯度爆炸需要降低学习率，过小(<0.01)说明梯度消失需要检查模型配置。学习率(Learning Rate)应该按照 warmup 策略变化，前 10%步数线性增加，然后线性衰减到 0。

训练中常见的问题及解决方案:显存不足时可以减小 batch_size 或 max_length，使用梯度累积或更小的模型;训练速度慢时可以增大 batch_size，减少 logging 频率，或使用混合精度训练;损失不下降时可以增大学习率，检查数据格式，或增加训练轮数;过拟合时可以增大 weight_decay，减少训练轮数，或使用更多数据。



### 6.3. 11.4 GRPO
在完成 SFT 训练后，我们已经得到了一个能够生成结构化答案的模型。但是，SFT 模型只是学会了"模仿"训练数据中的推理过程，并没有真正学会"思考"。强化学习可以让模型通过试错来优化推理策略，从而超越训练数据的质量。

#### 6.3.1. GRPO训练过程解析
1. a
2. KL散度乘法
   1. KL 散度惩罚是 GRPO 的关键组成部分，它防止策略偏离参考模型太远。

#### 6.3.2. GRPO 训练监控

* 平均奖励
* KL散度
* 准确率
* 生成质量

方式：
* Weight & Biases
* TensorBoard


### 6.4. 11.5 模型评估与分析

#### 6.4.1. 评估指标体系
**准确性指标**
准确性， top-k准确率， 数值误差
**效率指标**
平均长度， 推理步骤数，推理时间
**质量指标**
格式正确率，推理连贯性，可解释性

#### 6.4.2. 错误分析
模型的错误可以分为四类:
* 计算错误(推理步骤正确但计算出错，如"48/2=25"，说明数值计算能力不足)
* 推理错误(推理逻辑错误导致解题思路不对，如先加后除而非先除后加，说明逻辑推理能力不足)
* 理解错误(没有正确理解问题，如问题问"总共"但只计算了一部分，说明语言理解能力不足)
* 格式错误(答案正确但格式不符合要求，如缺少"Final Answer:"标记，说明格式学习不足)。

#### 6.4.3. 改进方向
![alt text](11-8.png)

### 6.5. 11.6 完整训练流程
#### 6.5.1. 端到端训练
从小规模开始, 数据质量检查

#### 6.5.2. 超参数调优

* **网格搜索**: 遍历所有参数组合，选择最佳的一组
* **随机搜索**: 随机搜索的优点是效率高，适合参数空间大的情况。缺点是可能错过最优解
* **贝叶斯优化**: 贝叶斯优化(Bayesian Optimization)使用概率模型指导搜索，更加智能。可以使用 Optuna 等库:

#### 6.5.3. 分布式训练
TRL 和 Hugging Face Accelerate，天然支持多 GPU 和多节点分布式训练
方案选择建议:

单机多卡(2-8 卡): 使用 DDP，简单高效 数据并行(DDP)
大模型(>7B): 使用 DeepSpeed ZeRO-2 或 ZeRO-3
多节点集群: 使用 DeepSpeed ZeRO-3 + Offload

##### 6.5.3.1. 配置Accelerate
##### 6.5.3.2. 使用DDP 训练

##### 6.5.3.3. 使用 DeepSpeed ZeRO 训练

DeepSpeed ZeRO通过分片优化器状态、梯度和模型参数，大幅降低显存占用，支持更大的模型和 batch size。